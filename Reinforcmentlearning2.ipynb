{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "#Planning\n",
        "\n",
        "doesn't involve necessarily going to the end goal rather planning the best action for a specific state and thats it.\n",
        "\n",
        "The below is an example for One-step Q planning"
      ],
      "metadata": {
        "id": "uUpBGKMcLfZ2"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uYEZgk-0C_1z",
        "outputId": "f429bb73-11b6-4ecc-a71d-5136ac8dc9f0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  1.]\n",
            " [ 0. -1.  0.]\n",
            " [ 0.  0.  0.]]\n",
            "['down', 'right']\n",
            "(0, 1)\n",
            "1.0\n"
          ]
        }
      ],
      "source": [
        "# prompt: create a 3x3 grid on which I can perform value iteration\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size=3):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))\n",
        "        # Example: Define some rewards (you can customize this)\n",
        "        self.grid[0, 2] = 1  # Goal state\n",
        "        self.grid[1, 1] = -1  # Obstacle or penalty\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        row, col = state\n",
        "        actions = []\n",
        "        if row > 0:\n",
        "            actions.append(\"up\")\n",
        "        if row < self.size - 1:\n",
        "            actions.append(\"down\")\n",
        "        if col > 0:\n",
        "            actions.append(\"left\")\n",
        "        if col < self.size - 1:\n",
        "            actions.append(\"right\")\n",
        "        return actions\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        row, col = state\n",
        "        if action == \"up\":\n",
        "            row -= 1\n",
        "        elif action == \"down\":\n",
        "            row += 1\n",
        "        elif action == \"left\":\n",
        "            col -= 1\n",
        "        elif action == \"right\":\n",
        "            col += 1\n",
        "        return row, col\n",
        "\n",
        "    def get_reward(self,state):\n",
        "        return self.grid[state]\n",
        "\n",
        "# Example usage\n",
        "grid = GridWorld()\n",
        "print(grid.grid)\n",
        "print(grid.get_possible_actions((0,0)))\n",
        "print(grid.get_next_state((0,0),\"right\"))\n",
        "print(grid.get_reward((0,2)))\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "Q = [{'up':0.0,'down':0.0,'right':0.0,'left':0.0} for _ in range(num_states)]\n",
        "N = [{'up':0,'down':0,'right':0,'left':0} for _ in range(num_states)]\n",
        "\n",
        "# Initialize pi(s) randomly\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0.5, \"left\": 0.5, \"right\": 0},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "\n",
        "print(\"Q(s,a):\", Q)\n",
        "print(\"pi(s):\", pi)\n",
        "print(N)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MLVI48dZLXb-",
        "outputId": "69f33fea-5e63-429a-eb2e-dae1ec39802e"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q(s,a): [{'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}]\n",
            "pi(s): {0: {'up': 0, 'down': 0.5, 'left': 0, 'right': 0.5}, 1: {'up': 0, 'down': 0.33, 'left': 0.33, 'right': 0.34}, 2: {'up': 0, 'down': 0.5, 'left': 0.5, 'right': 0}, 3: {'up': 0.34, 'down': 0.33, 'left': 0, 'right': 0.33}, 4: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}, 5: {'up': 0.33, 'down': 0.34, 'left': 0.33, 'right': 0}, 6: {'up': 0.5, 'down': 0, 'left': 0, 'right': 0.5}, 7: {'up': 0.33, 'down': 0, 'left': 0.33, 'right': 0.34}, 8: {'up': 0.5, 'down': 0, 'left': 0.5, 'right': 0}}\n",
            "[{'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 5e6\n",
        "gamma = 0.9\n",
        "for _ in range(int(episodes)):\n",
        "    # Generate an episode\n",
        "    state = random.randint(0,8)\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "    #print((state//3,state%3))\n",
        "    #print(action)\n",
        "    next_state = grid.get_next_state((state//3,state%3),action)\n",
        "    reward = grid.get_reward(next_state)\n",
        "    nstate = next_state[0]*3 + next_state[1]\n",
        "    N[state][action] += 1\n",
        "    value = None\n",
        "    for next_action in grid.get_possible_actions(next_state):\n",
        "        if value is None or Q[nstate][next_action] > value:\n",
        "            value = Q[nstate][next_action]\n",
        "    Q[state][action] += (1/N[state][action])*(reward + gamma*value - Q[state][action])"
      ],
      "metadata": {
        "id": "8hUXpyCALuY_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        value = Q[s][a]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hQrVhSvSLzTJ",
        "outputId": "890175ec-3693-4d22-988c-84eb9ff9bdd1"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy (Monte Carlo): ['right', 'right', 'down', 'up', 'up', 'up', 'right', 'right', 'up']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "Q"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B60GaB6dOs_B",
        "outputId": "f0d97c44-3fb3-40ab-b057-85db96a5b6bb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[{'up': 0.0,\n",
              "  'down': np.float64(2.8449885984042345),\n",
              "  'right': np.float64(3.7482476305608814),\n",
              "  'left': 0.0},\n",
              " {'up': 0.0,\n",
              "  'down': np.float64(2.2751158001799805),\n",
              "  'right': np.float64(4.274880558939775),\n",
              "  'left': np.float64(3.2735082183825375)},\n",
              " {'up': 0.0,\n",
              "  'down': np.float64(3.7486537879118593),\n",
              "  'right': 0.0,\n",
              "  'left': np.float64(3.7483413507711068)},\n",
              " {'up': np.float64(3.273452406758655),\n",
              "  'down': np.float64(2.463450863875035),\n",
              "  'right': np.float64(2.274840309082044),\n",
              "  'left': 0.0},\n",
              " {'up': np.float64(3.748575261726096),\n",
              "  'down': np.float64(2.84761751750847),\n",
              "  'right': np.float64(3.748149049949298),\n",
              "  'left': np.float64(2.8449468886457203)},\n",
              " {'up': np.float64(4.274606454481191),\n",
              "  'down': np.float64(3.2742210558975926),\n",
              "  'right': 0.0,\n",
              "  'left': np.float64(2.2752349704862675)},\n",
              " {'up': np.float64(2.8452890133044537),\n",
              "  'down': 0.0,\n",
              "  'right': np.float64(2.847761961214706),\n",
              "  'left': 0.0},\n",
              " {'up': np.float64(2.2744256118980553),\n",
              "  'down': 0.0,\n",
              "  'right': np.float64(3.2743205611937656),\n",
              "  'left': np.float64(2.463785410258508)},\n",
              " {'up': np.float64(3.747913191458152),\n",
              "  'down': 0.0,\n",
              "  'right': 0.0,\n",
              "  'left': np.float64(2.8477068196853286)}]"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Dyna Q"
      ],
      "metadata": {
        "id": "6jdd9j9EmRjo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "Q = [{'up':0.0,'down':0.0,'right':0.0,'left':0.0} for _ in range(num_states)]\n",
        "N = [{'up':0,'down':0,'right':0,'left':0} for _ in range(num_states)]\n",
        "Model = [{'up':(-2000,-2000),'down':(-2000,-2000),'right':(-2000,-2000),'left':(-2000,-2000)} for _ in range(num_states)]\n",
        "\n",
        "# Initialize pi(s) randomly\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0.5, \"left\": 0.5, \"right\": 0},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "\n",
        "print(\"Q(s,a):\", Q)\n",
        "print(\"pi(s):\", pi)\n",
        "print(N)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "maJHkGEnPgle",
        "outputId": "54079878-9693-4a2c-868a-a894b6b1f06b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q(s,a): [{'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}]\n",
            "pi(s): {0: {'up': 0, 'down': 0.5, 'left': 0, 'right': 0.5}, 1: {'up': 0, 'down': 0.33, 'left': 0.33, 'right': 0.34}, 2: {'up': 0, 'down': 0.5, 'left': 0.5, 'right': 0}, 3: {'up': 0.34, 'down': 0.33, 'left': 0, 'right': 0.33}, 4: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}, 5: {'up': 0.33, 'down': 0.34, 'left': 0.33, 'right': 0}, 6: {'up': 0.5, 'down': 0, 'left': 0, 'right': 0.5}, 7: {'up': 0.33, 'down': 0, 'left': 0.33, 'right': 0.34}, 8: {'up': 0.5, 'down': 0, 'left': 0.5, 'right': 0}}\n",
            "[{'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 1e6\n",
        "n=5\n",
        "gamma = 0.9\n",
        "#DYNA-Q\n",
        "for _ in range(int(episodes)):\n",
        "    '''\n",
        "    Choose a state and action\n",
        "    calculate the Q value for the state and action.\n",
        "\n",
        "    and update the Q value and store the next state and reward for the next state as a pair in the Model dictionary\n",
        "\n",
        "    DIRECT REINFORCEMENT LEARNING\n",
        "    '''\n",
        "    state = random.randint(0,8)\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "\n",
        "    next_state = grid.get_next_state((state//3,state%3),action)\n",
        "    reward = grid.get_reward(next_state)\n",
        "    nstate = next_state[0]*3 + next_state[1]\n",
        "    N[state][action] += 1\n",
        "    value = None\n",
        "    for next_action in grid.get_possible_actions(next_state):\n",
        "        if value is None or Q[nstate][next_action] > value:\n",
        "            value = Q[nstate][next_action]\n",
        "    Q[state][action] += (1/N[state][action])*(reward + gamma*value - Q[state][action])\n",
        "    Model[state][action] = (nstate,reward)\n",
        "    '''\n",
        "    select random past experiences and then calculate the Q value for those past experience\n",
        "    We are trying to learn the previous experience\n",
        "\n",
        "    INDIRECT REINFORCEMENT LEARNING\n",
        "    '''\n",
        "    for i in range(n):\n",
        "      iterate_state = random.randint(0,8)\n",
        "      actions, probabilities = zip(*pi[iterate_state].items())\n",
        "      iterate_action = (random.choices(actions, probabilities)[0])\n",
        "      iterate_next_state, iterate_reward = Model[iterate_state][iterate_action]\n",
        "\n",
        "      if iterate_next_state == -2000:\n",
        "        continue\n",
        "\n",
        "\n",
        "      N[state][action] += 1\n",
        "      value = None\n",
        "      for next_action in grid.get_possible_actions((iterate_next_state//3,iterate_next_state%3)):\n",
        "        if value is None or Q[iterate_next_state][iterate_action] > value:\n",
        "            value = Q[iterate_state][next_action]\n",
        "      Q[state][action] += (1/N[state][action])*(iterate_reward + gamma*value - Q[iterate_next_state][iterate_action])"
      ],
      "metadata": {
        "id": "fqJ-IAuGmk1s"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        value = Q[s][a]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "soaj0HzRojcA",
        "outputId": "6e2a4036-109c-4b77-c6b2-c5e748102bc0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy (Monte Carlo): ['right', 'right', 'down', 'up', 'right', 'up', 'right', 'right', 'up']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Prioritized Sweeping\n",
        "\n",
        "we perform operations and update our Fixed model only when the priortized value is met given a threshol|\n",
        "\n",
        "Goal : Minimize the number of exploration for reaching an optimal solution\n",
        "Make the calculation efficient to scale to a larger model"
      ],
      "metadata": {
        "id": "_E_1zRVYsyK8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "\n",
        "import random\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "Q = [{'up':0.0,'down':0.0,'right':0.0,'left':0.0} for _ in range(num_states)]\n",
        "N = [{'up':0,'down':0,'right':0,'left':0} for _ in range(num_states)]\n",
        "Model = [{'up':(-2000,-2000),'down':(-2000,-2000),'right':(-2000,-2000),'left':(-2000,-2000)} for _ in range(num_states)]\n",
        "\n",
        "# Initialize pi(s) randomly\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0.5, \"left\": 0.5, \"right\": 0},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "\n",
        "print(\"Q(s,a):\", Q)\n",
        "print(\"pi(s):\", pi)\n",
        "print(N)\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UL3oHlA1o01_",
        "outputId": "75a52982-5a53-4c0e-a59a-373ca4d70e5a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Q(s,a): [{'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}, {'up': 0.0, 'down': 0.0, 'right': 0.0, 'left': 0.0}]\n",
            "pi(s): {0: {'up': 0, 'down': 0.5, 'left': 0, 'right': 0.5}, 1: {'up': 0, 'down': 0.33, 'left': 0.33, 'right': 0.34}, 2: {'up': 0, 'down': 0.5, 'left': 0.5, 'right': 0}, 3: {'up': 0.34, 'down': 0.33, 'left': 0, 'right': 0.33}, 4: {'up': 0.25, 'down': 0.25, 'left': 0.25, 'right': 0.25}, 5: {'up': 0.33, 'down': 0.34, 'left': 0.33, 'right': 0}, 6: {'up': 0.5, 'down': 0, 'left': 0, 'right': 0.5}, 7: {'up': 0.33, 'down': 0, 'left': 0.33, 'right': 0.34}, 8: {'up': 0.5, 'down': 0, 'left': 0.5, 'right': 0}}\n",
            "[{'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}, {'up': 0, 'down': 0, 'right': 0, 'left': 0}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from queue import PriorityQueue\n",
        "\n",
        "pqchange = PriorityQueue()"
      ],
      "metadata": {
        "id": "4RVITbnCtycY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "episodes = 1e6\n",
        "n=5\n",
        "gamma = 0.9\n",
        "threshold = 0.05\n",
        "#Prioritzed sweeping(Not completed (Boring))\n",
        "for _ in range(int(episodes)):\n",
        "    '''\n",
        "    Choose a state and action\n",
        "    calculate the Q value for the state and action.\n",
        "\n",
        "    and update the Q value and store the next state and reward for the next state as a pair in the Model dictionary\n",
        "\n",
        "    DIRECT REINFORCEMENT LEARNING\n",
        "    '''\n",
        "    state = random.randint(0,8)\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "\n",
        "    next_state = grid.get_next_state((state//3,state%3),action)\n",
        "    reward = grid.get_reward(next_state)\n",
        "    nstate = next_state[0]*3 + next_state[1]\n",
        "\n",
        "    Model[state][action] = (nstate,reward)\n",
        "\n",
        "    value = None\n",
        "    for next_action in grid.get_possible_actions(next_state):\n",
        "        if value is None or Q[nstate][next_action] > value:\n",
        "            value = Q[nstate][next_action]\n",
        "    P = abs(reward + gamma*value - Q[state][action])\n",
        "    if P>threshold:\n",
        "      pqchange.put((P,(state,action)))\n",
        "\n",
        "\n",
        "    '''\n",
        "    select random past experiences and then calculate the Q value for those past experience\n",
        "    We are trying to learn the previous experience\n",
        "\n",
        "    INDIRECT REINFORCEMENT LEARNING\n",
        "    '''\n",
        "    for i in range(n):\n",
        "      if pqchange.empty():\n",
        "        break\n",
        "      iterate_state , iterate_action = pqchange.get()[1]\n",
        "\n",
        "      iterate_next_state, iterate_reward = Model[iterate_state][iterate_action]\n",
        "\n",
        "\n",
        "      N[state][action] += 1\n",
        "      value = None\n",
        "      for next_action in grid.get_possible_actions((iterate_next_state//3,iterate_next_state%3)):\n",
        "        if value is None or Q[iterate_next_state][iterate_action] > value:\n",
        "            value = Q[iterate_state][next_action]\n",
        "      Q[state][action] += (1/N[state][action])*(iterate_reward + gamma*value - Q[iterate_next_state][iterate_action])\n",
        "\n",
        "      ##Another step to map from next_state to state\n",
        "      #samething.\n"
      ],
      "metadata": {
        "id": "5s0GmDg_tADO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "##Chapter 9 : Gradient descent stuff\n",
        "\n",
        "- gradient monte carlo algorithm\n"
      ],
      "metadata": {
        "id": "Zle-iuagzdE_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a 3x3 grid on which I can perform value iteration\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size=3):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))\n",
        "        # Example: Define some rewards (you can customize this)\n",
        "        self.grid[0, 2] = 1  # Goal state\n",
        "        self.grid[1, 1] = -1  # Obstacle or penalty\n",
        "        self.grid[2,2] = -1\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        row, col = state\n",
        "        actions = []\n",
        "        if row > 0:\n",
        "            actions.append(\"up\")\n",
        "        if row < self.size - 1:\n",
        "            actions.append(\"down\")\n",
        "        if col > 0:\n",
        "            actions.append(\"left\")\n",
        "        if col < self.size - 1:\n",
        "            actions.append(\"right\")\n",
        "        return actions\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        row, col = state\n",
        "        if action == \"up\":\n",
        "            row -= 1\n",
        "        elif action == \"down\":\n",
        "            row += 1\n",
        "        elif action == \"left\":\n",
        "            col -= 1\n",
        "        elif action == \"right\":\n",
        "            col += 1\n",
        "        return row, col\n",
        "\n",
        "    def get_reward(self,state):\n",
        "        return self.grid[state]\n",
        "\n",
        "# Example usage\n",
        "grid = GridWorld()\n",
        "print(grid.grid)\n",
        "print(grid.get_possible_actions((0,0)))\n",
        "print(grid.get_next_state((0,0),\"right\"))\n",
        "print(grid.get_reward((0,2)))\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Zgl496oYzl-5",
        "outputId": "3b880e54-7bf8-4aef-88ba-8d55be1e2b03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  1.]\n",
            " [ 0. -1.  0.]\n",
            " [ 0.  0. -1.]]\n",
            "['down', 'right']\n",
            "(0, 1)\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "#V = [0 for _ in range(num_states)]\n",
        "#N = [0 for _ in range(num_states)]\n",
        "\n",
        "W = np.zeros((3))\n",
        "#winning strategy\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 1.00},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "# Initialize pi(s) randomly\n",
        "\n",
        "\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7dBlqS6c0oG2",
        "outputId": "bab989fe-30b3-467a-adff-4bf6ec458305"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "alpha = 0.05\n",
        "\n",
        "while i<100000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "  episodes = []\n",
        "  actionlist = []\n",
        "  goallist = []\n",
        "  i+=1\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "    random.choices(actions, probabilities)\n",
        "    #N[state]+=1\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "    #V[state] = V[state] + (1/N[state])*(reward + gamma*V[newstate]-V[state])\n",
        "    episodes = episodes + [state]\n",
        "    actionlist = actionlist + [action]\n",
        "    goallist = goallist + [reward]\n",
        "    state = newstate\n",
        "\n",
        "    if state == goal:\n",
        "      break\n",
        "\n",
        "  G=0\n",
        "  for t in reversed(range(len(episodes))):\n",
        "    state = episodes[t]\n",
        "    action = actionlist[t]\n",
        "    reward = goallist[t]\n",
        "\n",
        "    G = gamma*G + reward\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "    V = W.dot(np.array([x,y,1]))\n",
        "    W += alpha*(G - V)*np.array([x,y,1])\n",
        "    #N[state][action]+=1\n",
        "    #Q[state][action] = Q[state][action] + (1/N[state][action])*(G-Q[state][action])\n",
        "    #for doable in grid.get_possible_actions((state//3,state%3)):\n",
        "    #  if Q[state][doable] > Q[state][action]:\n",
        "    #    action = doable\n",
        "    #pi[state] = action\n"
      ],
      "metadata": {
        "id": "GZvKtski8jZX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = []\n",
        "for state in range(9):\n",
        "  x = state//3\n",
        "  y = state%3\n",
        "  V.append(W.dot(np.array([x,y,1])))\n",
        "\n",
        "V\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LVJR73xQJDYc",
        "outputId": "abc78f7c-626d-4d3f-b874-5169191b0512"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[np.float64(-0.5439965777805723),\n",
              " np.float64(-0.4855123284781801),\n",
              " np.float64(-0.4270280791757879),\n",
              " np.float64(-0.6862892970697911),\n",
              " np.float64(-0.627805047767399),\n",
              " np.float64(-0.5693207984650067),\n",
              " np.float64(-0.8285820163590101),\n",
              " np.float64(-0.7700977670566178),\n",
              " np.float64(-0.7116135177542257)]"
            ]
          },
          "metadata": {},
          "execution_count": 21
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        s_prime = grid.get_next_state((s // 3, s % 3), a)\n",
        "        value = grid.get_reward(s_prime) + gamma * V[s_prime[0] * 3 + s_prime[1]]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)\n",
        "print(\"Weight:\",W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OaZy-LqwJhPt",
        "outputId": "f89521a6-6cba-4ec6-a120-87de1199b322"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy (Monte Carlo): ['right', 'right', 'left', 'up', 'up', 'up', 'up', 'left', 'up']\n",
            "Weight: [-0.14229272  0.05848425 -0.54399658]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TD(0) approach using semi gradient\n",
        "\n",
        "same approach but instead of using the goal we calculate TD(0) = R+ gamma*V with our neural network function to coompute the same"
      ],
      "metadata": {
        "id": "Fms7AL6bLGTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "#V = [0 for _ in range(num_states)]\n",
        "#N = [0 for _ in range(num_states)]\n",
        "\n",
        "W = np.zeros((3))\n",
        "#winning strategy\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 1.00},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "# Initialize pi(s) randomly\n",
        "\n",
        "\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0IHWTDHLLFbB",
        "outputId": "a8885350-792f-423b-8091-a8f273e25ad3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "alpha = 0.05\n",
        "\n",
        "while i<100000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "  i+=1\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "    random.choices(actions, probabilities)\n",
        "    #N[state]+=1\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "\n",
        "    V_next = W.dot(np.array([newx,newy,1]))\n",
        "    V = W.dot(np.array([x,y,1]))\n",
        "\n",
        "    W += alpha*(reward + gamma*V_next - V)*np.array([x,y,1])\n",
        "\n",
        "    state = newstate\n",
        "\n",
        "    if state == goal:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "m5AyI17-LWPE"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = []\n",
        "for state in range(9):\n",
        "  x = state//3\n",
        "  y = state%3\n",
        "  V.append(W.dot(np.array([x,y,1])))\n",
        "\n",
        "print(\"Value Function (TD(0)):\", V)\n",
        "\n",
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        s_prime = grid.get_next_state((s // 3, s % 3), a)\n",
        "        value = grid.get_reward(s_prime) + gamma * V[s_prime[0] * 3 + s_prime[1]]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)\n",
        "print(\"Weight:\",W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7T-5NUAjMi4y",
        "outputId": "2ab75fbb-e38f-4d3e-fe67-63e236905599"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function (TD(0)): [np.float64(-1.0724578835080645), np.float64(-0.9533612908049747), np.float64(-0.834264698101885), np.float64(-1.2770244004521643), np.float64(-1.1579278077490744), np.float64(-1.0388312150459846), np.float64(-1.481590917396264), np.float64(-1.3624943246931742), np.float64(-1.2433977319900844)]\n",
            "Optimal policy (Monte Carlo): ['right', 'right', 'left', 'up', 'up', 'up', 'up', 'left', 'up']\n",
            "Weight: [-0.20456652  0.11909659 -1.07245788]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "You can extend The above algorithm to N-steps as well\n",
        "\n",
        "\n",
        "##Chapter 10: On policy control (Using Q values) for Approximation\n",
        "\n"
      ],
      "metadata": {
        "id": "aLNGZJZ8fgq5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "#V = [0 for _ in range(num_states)]\n",
        "#N = [0 for _ in range(num_states)]\n",
        "\n",
        "W = np.zeros((4))\n",
        "\n",
        "actiondict1 = {'up':1 , 'down':-1 , 'left':0 , 'right':0}\n",
        "actiondict2 = {'up':0 , 'down':0 , 'left':-1 , 'right':1}\n",
        "#winning strategy\n",
        "\n",
        "\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LCd2d4x1MvCD",
        "outputId": "f6962b8a-6b17-4f7c-ad99-cf386c35253c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0., 0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 65
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def best_action(state):\n",
        "  action = None\n",
        "  x = state//3\n",
        "  y = state%3\n",
        "  if random.random() < epsilon:\n",
        "    action = random.choice(grid.get_possible_actions((x,y)))\n",
        "  else:\n",
        "    buff_value = None\n",
        "    for a in grid.get_possible_actions((x,y)):\n",
        "      next_state = grid.get_next_state((x,y),a)\n",
        "      if action is None or W.dot(np.array([newx,newy,actiondict1[a],actiondict2[a]]))>buff_value:\n",
        "        action = a\n",
        "        buff_value = W.dot(np.array([newx,newy,actiondict1[a],actiondict2[a]]))\n",
        "  return action"
      ],
      "metadata": {
        "id": "OHH6NT5tvNFv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "alpha = 0.01\n",
        "epsilon = 0.2\n",
        "\n",
        "while i<100000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "  i+=1\n",
        "\n",
        "  action = best_action(state)\n",
        "\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "\n",
        "\n",
        "    currentQ =W.dot(np.array([x,y,actiondict1[action],actiondict2[action]]))\n",
        "\n",
        "\n",
        "    if state == goal:\n",
        "      W += alpha*(reward  - currentQ)*np.array([x,y,actiondict1[action],actiondict2[action]])\n",
        "      break\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "\n",
        "    newaction = best_action(newstate)\n",
        "    nextQ = W.dot(np.array([newx,newy,actiondict1[newaction],actiondict2[newaction]]))\n",
        "    W += alpha*(reward + gamma*nextQ - currentQ)*np.array([x,y,actiondict1[action],actiondict2[action]])\n",
        "\n",
        "    state = newstate\n",
        "    action = newaction"
      ],
      "metadata": {
        "id": "txO9H85QsxP1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "Q = [{'up':0.0,'down':0.0,'right':0.0,'left':0.0} for _ in range(num_states)]\n",
        "\n",
        "for state in range(num_states):\n",
        "  for action in grid.get_possible_actions((state//3,state%3)):\n",
        "    Q[state][action] = W.dot(np.array([state//3,state%3,actiondict1[action],actiondict2[action]]))"
      ],
      "metadata": {
        "id": "JHX-bkpCw5va"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Maybe the features should be modified for a better result, but this is what I could come up with . Playing around with the reward , by boosting up the negative reward can get the desired results"
      ],
      "metadata": {
        "id": "-YqhSC2c0Rg3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        value = Q[s][a]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)\n",
        "print(Q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XWBumZ6LwA9l",
        "outputId": "d14aa762-00aa-4968-8116-dec3d474cb90"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Optimal policy (Monte Carlo): ['right', 'right', 'down', 'right', 'right', 'up', 'right', 'right', 'up']\n",
            "[{'up': 0.0, 'down': np.float64(-0.6535574725522957), 'right': np.float64(0.9472149871672468), 'left': 0.0}, {'up': 0.0, 'down': np.float64(0.2213809055655951), 'right': np.float64(1.8221533652851376), 'left': np.float64(-0.07227660904935607)}, {'up': 0.0, 'down': np.float64(1.0963192836834859), 'right': 0.0, 'left': np.float64(0.8026617690685347)}, {'up': np.float64(0.2367900811781281), 'down': np.float64(-1.0703248639264633), 'right': np.float64(0.5304475957930792), 'left': 0.0}, {'up': np.float64(1.1117284592960188), 'down': np.float64(-0.19538648580857249), 'right': np.float64(1.40538597391097), 'left': np.float64(-0.48904400042352364)}, {'up': np.float64(1.9866668374139096), 'down': np.float64(0.6795518923093182), 'right': 0.0, 'left': np.float64(0.3858943776943671)}, {'up': np.float64(-0.17997731019603946), 'down': 0.0, 'right': np.float64(0.11368020441891169), 'left': 0.0}, {'up': np.float64(0.6949610679218513), 'down': 0.0, 'right': np.float64(0.9886185825368025), 'left': np.float64(-0.9058113917976912)}, {'up': np.float64(1.569899446039742), 'down': 0.0, 'right': 0.0, 'left': np.float64(-0.030873013679800443)}]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "The above algorithm can be extended toward n-step SARSA!!\n",
        "\n",
        "There is an expected approach for Sarsa for continuous reward which replaces gamma discounting with R mean\n",
        "\n",
        "delta = summation(t+1 -> t+n) - (R- Rmean) + Q(nextstate,a') - Q(state,a)\n",
        "R_mean = R_mean  + beta * delta\n",
        "beta is some contstant> 0"
      ],
      "metadata": {
        "id": "7aQiqGSv0nej"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "##TD(lambda)"
      ],
      "metadata": {
        "id": "X_zxV-7THJCb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: create a 3x3 grid on which I can perform value iteration\n",
        "\n",
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    def __init__(self, size=3):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))\n",
        "        # Example: Define some rewards (you can customize this)\n",
        "        self.grid[0, 2] = 1  # Goal state\n",
        "        self.grid[1, 1] = -1  # Obstacle or penalty\n",
        "        self.grid[2,2] = -1\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        row, col = state\n",
        "        actions = []\n",
        "        if row > 0:\n",
        "            actions.append(\"up\")\n",
        "        if row < self.size - 1:\n",
        "            actions.append(\"down\")\n",
        "        if col > 0:\n",
        "            actions.append(\"left\")\n",
        "        if col < self.size - 1:\n",
        "            actions.append(\"right\")\n",
        "        return actions\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        row, col = state\n",
        "        if action == \"up\":\n",
        "            row -= 1\n",
        "        elif action == \"down\":\n",
        "            row += 1\n",
        "        elif action == \"left\":\n",
        "            col -= 1\n",
        "        elif action == \"right\":\n",
        "            col += 1\n",
        "        return row, col\n",
        "\n",
        "    def get_reward(self,state):\n",
        "        return self.grid[state]\n",
        "\n",
        "# Example usage\n",
        "grid = GridWorld()\n",
        "print(grid.grid)\n",
        "print(grid.get_possible_actions((0,0)))\n",
        "print(grid.get_next_state((0,0),\"right\"))\n",
        "print(grid.get_reward((0,2)))\n"
      ],
      "metadata": {
        "id": "WxsGqsnYw3ah",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "67ff42b3-133b-4cdd-cd1d-b63468d3005d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  1.]\n",
            " [ 0. -1.  0.]\n",
            " [ 0.  0. -1.]]\n",
            "['down', 'right']\n",
            "(0, 1)\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "#V = [0 for _ in range(num_states)]\n",
        "#N = [0 for _ in range(num_states)]\n",
        "\n",
        "W = np.zeros((2))\n",
        "#winning strategy\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 1.00},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "# Initialize pi(s) randomly\n",
        "\n",
        "\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yjcVuyhuHQgv",
        "outputId": "4d297851-fad8-4ce3-e8b4-2d702783b61b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "lamb_da = 0.8\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "alpha = 0.05\n",
        "\n",
        "while i<100000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "  i+=1\n",
        "  Z = np.zeros((2))\n",
        "\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "    random.choices(actions, probabilities)\n",
        "    #N[state]+=1\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "\n",
        "    V_next = W.dot(np.array([newx,newy]))\n",
        "\n",
        "    Z = gamma*lamb_da*Z + np.array([x,y])\n",
        "\n",
        "    V = W.dot(np.array([x,y]))\n",
        "\n",
        "    W += alpha*(reward + gamma*V_next - V)*Z\n",
        "\n",
        "    state = newstate\n",
        "\n",
        "    if state == goal:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "ARI_Hf6lH70l"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ogePPXgQVkoJ",
        "outputId": "3fbde045-25d6-4f42-d0a7-3c15a8e3249d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([-0.35977965,  0.19676802])"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "V = []\n",
        "for state in range(9):\n",
        "  x = state//3\n",
        "  y = state%3\n",
        "  V.append(W.dot(np.array([x,y])))\n",
        "\n",
        "print(\"Value Function (TD(0)):\", V)\n",
        "\n",
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        s_prime = grid.get_next_state((s // 3, s % 3), a)\n",
        "        value = grid.get_reward(s_prime) + gamma * V[s_prime[0] * 3 + s_prime[1]]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)\n",
        "print(\"Weight:\",W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PEerEzjFWprS",
        "outputId": "3b7ac312-5f78-4d2f-bdc3-701ebbeb8a91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function (TD(0)): [np.float64(0.0), np.float64(0.19676801761779486), np.float64(0.3935360352355897), np.float64(-0.3597796527713165), np.float64(-0.16301163515352166), np.float64(0.033756382464273205), np.float64(-0.719559305542633), np.float64(-0.5227912879248382), np.float64(-0.3260232703070433)]\n",
            "Optimal policy (Monte Carlo): ['right', 'right', 'left', 'up', 'up', 'up', 'up', 'left', 'up']\n",
            "Weight: [-0.35977965  0.19676802]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Online TD(lambda)"
      ],
      "metadata": {
        "id": "LjoXhjfXpLtc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import random\n",
        "import numpy as np\n",
        "\n",
        "# Assuming a 3x3 grid world as an example\n",
        "num_states = 9  # Number of states in the grid\n",
        "num_actions = 4  # Number of possible actions (up, down, left, right)\n",
        "\n",
        "# Initialize V(s) randomly\n",
        "#V = [0 for _ in range(num_states)]\n",
        "#N = [0 for _ in range(num_states)]\n",
        "\n",
        "W = np.zeros((2))\n",
        "#winning strategy\n",
        "pi = target_policy = {\n",
        "    0: {\"up\": 0, \"down\": 0.5, \"left\": 0, \"right\": 0.5},  # (0,0) → mostly right\n",
        "    1: {\"up\": 0, \"down\": 0.33, \"left\": 0.33, \"right\": 0.34},  # (0,1) → mostly right\n",
        "    2: {\"up\": 0, \"down\": 0, \"left\": 0, \"right\": 1.00},  # (0,2) → goal state\n",
        "    3: {\"up\": 0.34, \"down\": 0.33, \"left\": 0, \"right\": 0.33},  # (1,0) → mostly up\n",
        "    4: {\"up\": 0.25, \"down\": 0.25, \"left\": 0.25, \"right\": 0.25},  # (1,1) → penalty (random)\n",
        "    5: {\"up\": 0.33, \"down\": 0.34, \"left\": 0.33, \"right\": 0},  # (1,2) → mostly up\n",
        "    6: {\"up\": 0.5, \"down\": 0, \"left\": 0, \"right\": 0.5},  # (2,0) → mostly up\n",
        "    7: {\"up\": 0.33, \"down\": 0, \"left\": 0.33, \"right\": 0.34},  # (2,1) → mostly up\n",
        "    8: {\"up\": 0.5, \"down\": 0, \"left\": 0.5, \"right\": 0},  # (2,2) → mostly left\n",
        "}\n",
        "# Initialize pi(s) randomly\n",
        "\n",
        "\n",
        "W"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "P9aDBROopLd4",
        "outputId": "758bd720-7505-4d03-804c-faa396a5b0ff"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([0., 0.])"
            ]
          },
          "metadata": {},
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "lamb_da = 0.8\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "alpha = 0.05\n",
        "\n",
        "while i<100000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "  i+=1\n",
        "  Z = np.zeros((2))\n",
        "  V_old = 0\n",
        "\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "\n",
        "    actions, probabilities = zip(*pi[state].items())\n",
        "    action = (random.choices(actions, probabilities)[0])\n",
        "    random.choices(actions, probabilities)\n",
        "    #N[state]+=1\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "    V = W.dot(np.array([x,y]))\n",
        "    V_next = W.dot(np.array([newx,newy]))\n",
        "\n",
        "    delta = reward + gamma*V_next - V\n",
        "\n",
        "    Z = gamma*lamb_da*Z + (1 - alpha* gamma * lamb_da * Z.dot(np.array([x,y])))*np.array([x,y])\n",
        "\n",
        "    W += alpha*(delta + V - V_old)*Z - alpha*(V-V_old)*np.array([x,y])\n",
        "\n",
        "    V_old = V\n",
        "\n",
        "    state = newstate\n",
        "\n",
        "    if state == goal:\n",
        "      break\n"
      ],
      "metadata": {
        "id": "EO2-VEEIWwRd"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "V = []\n",
        "for state in range(9):\n",
        "  x = state//3\n",
        "  y = state%3\n",
        "  V.append(W.dot(np.array([x,y])))\n",
        "\n",
        "print(\"Value Function (TD(0)):\", V)\n",
        "\n",
        "pi_mc = []\n",
        "for s in range(num_states):\n",
        "    best_action = None\n",
        "    best_value = -float('inf')\n",
        "    for a in grid.get_possible_actions((s // 3, s % 3)):\n",
        "        s_prime = grid.get_next_state((s // 3, s % 3), a)\n",
        "        value = grid.get_reward(s_prime) + gamma * V[s_prime[0] * 3 + s_prime[1]]\n",
        "        if value > best_value:\n",
        "            best_value = value\n",
        "            best_action = a\n",
        "    pi_mc.append(best_action)\n",
        "\n",
        "print(\"Optimal policy (Monte Carlo):\", pi_mc)\n",
        "print(\"Weight:\",W)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4fdCyxXItrIk",
        "outputId": "337c68b8-62ad-4281-a7a4-6967ca856e4a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Value Function (TD(0)): [np.float64(0.0), np.float64(-0.04182715310722768), np.float64(-0.08365430621445535), np.float64(-0.3445419060423065), np.float64(-0.3863690591495342), np.float64(-0.42819621225676185), np.float64(-0.689083812084613), np.float64(-0.7309109651918406), np.float64(-0.7727381182990684)]\n",
            "Optimal policy (Monte Carlo): ['right', 'right', 'left', 'up', 'up', 'up', 'up', 'left', 'up']\n",
            "Weight: [-0.34454191 -0.04182715]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Sarsa(lambda)\n",
        "\n",
        "same thing but implementing sarsa W = 3dimension an extra for action\n",
        "\n",
        "similar approach as the other lambdas where our lambda update on z = lambda * gamma * z'\\\n",
        "\n",
        "There is also a true online SARSA approach ."
      ],
      "metadata": {
        "id": "kX6Ke8ISv911"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Policy Gradient"
      ],
      "metadata": {
        "id": "WTFZxO37ePQ9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Doesn't work. Have to modify it for a better result. It fails at state 7 doesn't give a good result at that state.\n",
        "\n",
        "\n",
        "FEATURES are the concern. We don't have much info to learn from . We can use conv net of the grid for more features but i'm not going to. The problem doesn't require Neural Network so Cool!!"
      ],
      "metadata": {
        "id": "wjwxJCWtvOpB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import random"
      ],
      "metadata": {
        "id": "f_FEyIkixC1S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=2, output_dim=4, hidden_dim=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x"
      ],
      "metadata": {
        "id": "cXWz_7hcxD2Y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "class GridWorld:\n",
        "    # Your existing GridWorld class remains unchanged\n",
        "    def __init__(self, size=3):\n",
        "        self.size = size\n",
        "        self.grid = np.zeros((size, size))\n",
        "        self.grid[0, 2] = 1  # Goal state\n",
        "        self.grid[1, 1] = -1  # Obstacle\n",
        "        self.grid[2,2] = -1\n",
        "\n",
        "\n",
        "    def get_possible_actions(self, state):\n",
        "        row, col = state\n",
        "        actions = []\n",
        "        if row > 0:\n",
        "            actions.append(\"up\")\n",
        "        if row < self.size - 1:\n",
        "            actions.append(\"down\")\n",
        "        if col > 0:\n",
        "            actions.append(\"left\")\n",
        "        if col < self.size - 1:\n",
        "            actions.append(\"right\")\n",
        "        return actions\n",
        "\n",
        "    def get_next_state(self, state, action):\n",
        "        row, col = state\n",
        "        if action == \"up\":\n",
        "            row -= 1\n",
        "        elif action == \"down\":\n",
        "            row += 1\n",
        "        elif action == \"left\":\n",
        "            col -= 1\n",
        "        elif action == \"right\":\n",
        "            col += 1\n",
        "\n",
        "\n",
        "        if row < 0 or row >= self.size or col < 0 or col >= self.size:\n",
        "            return state\n",
        "        else:\n",
        "           return row, col\n",
        "\n",
        "    def get_reward(self, state):\n",
        "        return self.grid[state]\n",
        "\n",
        "grid = GridWorld()\n",
        "print(grid.grid)\n",
        "print(grid.get_possible_actions((0,0)))\n",
        "print(grid.get_next_state((0,0),\"right\"))\n",
        "print(grid.get_reward((0,2)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H-D_IbG5kNmR",
        "outputId": "a4d51590-6fee-44bd-91a0-6a09bf68aeb6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[ 0.  0.  1.]\n",
            " [ 0. -1.  0.]\n",
            " [ 0.  0. -1.]]\n",
            "['down', 'right']\n",
            "(0, 1)\n",
            "1.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = PolicyNetwork()\n",
        "optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
        "action_map = [\"up\", \"down\", \"left\", \"right\"]"
      ],
      "metadata": {
        "id": "wwS_geSDxOer"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "alpha = 0.05\n",
        "\n",
        "while i<15000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "  episodes = []\n",
        "  actionlist = []\n",
        "  goallist = []\n",
        "  logproblist = []\n",
        "\n",
        "  i+=1\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "    state_tensor = torch.tensor((x,y), dtype=torch.float32)\n",
        "    action_probs = policy_net(state_tensor)\n",
        "    action_dist = torch.distributions.Categorical(action_probs) #categorical keeps track of action probs , log probs .\n",
        "    action_index  = action_dist.sample() #sample action\n",
        "    action = action_map[action_index.item()]\n",
        "    log_prob = action_dist.log_prob(action_index)#log probability\n",
        "\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "    episodes = episodes + [state]\n",
        "    actionlist = actionlist + [action]\n",
        "    goallist = goallist + [reward]\n",
        "    logproblist = logproblist + [log_prob]\n",
        "    state = newstate\n",
        "\n",
        "    if state == goal:\n",
        "      break\n",
        "\n",
        "  G=0\n",
        "  policy_loss = []\n",
        "  for t in reversed(range(len(episodes))):\n",
        "    state = episodes[t]\n",
        "    action = actionlist[t]\n",
        "    reward = goallist[t]\n",
        "    log_prob = logproblist[t]\n",
        "\n",
        "\n",
        "    G = gamma*G + reward\n",
        "    policy_loss.append(-log_prob*G)\n",
        "  optimizer.zero_grad()\n",
        "  loss = torch.stack(policy_loss).sum()\n",
        "  if i%1000==0:\n",
        "    print(f\"{i} episodes \\|\\| Loss : {loss}\")\n",
        "  loss.backward()\n",
        "  optimizer.step()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "baMz50wczocv",
        "outputId": "1e249319-0dad-4fdc-89db-88c3fea200f3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000 episodes \\|\\| Loss : 2.3841860752327193e-07\n",
            "2000 episodes \\|\\| Loss : 2.2649767572602286e-07\n",
            "3000 episodes \\|\\| Loss : 4.732609113489161e-07\n",
            "4000 episodes \\|\\| Loss : 2.2649767572602286e-07\n",
            "5000 episodes \\|\\| Loss : 1.1920930376163597e-07\n",
            "6000 episodes \\|\\| Loss : 2.0384790389016416e-07\n",
            "7000 episodes \\|\\| Loss : 2.2649767572602286e-07\n",
            "8000 episodes \\|\\| Loss : 3.2305720765180013e-07\n",
            "9000 episodes \\|\\| Loss : 1.1920930376163597e-07\n",
            "10000 episodes \\|\\| Loss : 2.2649767572602286e-07\n",
            "11000 episodes \\|\\| Loss : 2.2649767572602286e-07\n",
            "12000 episodes \\|\\| Loss : 1.1920930376163597e-07\n",
            "13000 episodes \\|\\| Loss : 2.0384790389016416e-07\n",
            "14000 episodes \\|\\| Loss : 2.0384790389016416e-07\n",
            "15000 episodes \\|\\| Loss : 2.0384790389016416e-07\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "  x = i//3\n",
        "  y = i%3\n",
        "  state_tensor = torch.tensor((x,y), dtype=torch.float32)\n",
        "  action_probs = policy_net(state_tensor)\n",
        "\n",
        "  print(i)\n",
        "  print('----------------------------------------------')\n",
        "  print(\"Best Action :\", action_map[np.argmax(action_probs.detach().numpy())])\n",
        "  print(action_probs)\n",
        "  #print(action_probs)\n",
        "  #print(action_dist.logits)\n",
        "  #print(action_dist.probs)\n",
        "  #print(action_index)\n",
        "\n",
        "  #print(action)\n",
        "  #print(log_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ytXaIaFJCpFd",
        "outputId": "ed444183-9746-4f2c-e7c3-bde19c6d6b3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "----------------------------------------------\n",
            "Best Action : right\n",
            "tensor([7.1313e-08, 8.4415e-09, 9.1579e-09, 1.0000e+00],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "1\n",
            "----------------------------------------------\n",
            "Best Action : right\n",
            "tensor([4.2570e-08, 4.5811e-10, 3.8859e-10, 1.0000e+00],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "2\n",
            "----------------------------------------------\n",
            "Best Action : right\n",
            "tensor([4.7620e-08, 2.4785e-11, 1.5792e-11, 1.0000e+00],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "3\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([1.0000e+00, 1.5127e-08, 6.7268e-09, 8.9378e-08],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "4\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([1.0000e+00, 8.3336e-10, 2.5925e-10, 6.9470e-08],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "5\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([1.0000e+00, 4.5106e-11, 9.7784e-12, 6.0992e-08],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "6\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([1.0000e+00, 5.3558e-16, 9.0141e-17, 1.1143e-15],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "7\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([1.0000e+00, 3.6853e-17, 4.4493e-18, 8.7026e-17],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "8\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([1.0000e+00, 2.5423e-18, 2.2186e-19, 6.9249e-18],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ACTOR -CRITIC\n",
        "\n",
        "Fancy term for using weight for value prediction and policy prediction"
      ],
      "metadata": {
        "id": "1pzaJ1cXNh5b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class PolicyNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=2, output_dim=4, hidden_dim=16):\n",
        "        super(PolicyNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, hidden_dim)\n",
        "        self.fc2 = nn.Linear(hidden_dim, output_dim)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = torch.relu(self.fc1(state))\n",
        "        x = self.softmax(self.fc2(x))\n",
        "        return x\n",
        "\n",
        "class ValueNetwork(nn.Module):\n",
        "    def __init__(self, input_dim=2, output_dim=1, hidden_dim=9):\n",
        "        super(ValueNetwork, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_dim, output_dim)\n",
        "\n",
        "    def forward(self, state):\n",
        "        x = self.fc1(state)\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "28tq018INyYY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "policy_net = PolicyNetwork()\n",
        "value_net =  ValueNetwork()\n",
        "optimizer1 = optim.Adam(policy_net.parameters(), lr=0.00005)\n",
        "optimizer2 = optim.Adam(value_net.parameters(), lr=0.01)\n",
        "action_map = [\"up\", \"down\", \"left\", \"right\"]"
      ],
      "metadata": {
        "id": "JVLHb1u1w_S2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "gamma = 0.9\n",
        "i=0\n",
        "weightsum=0\n",
        "goal =2\n",
        "\n",
        "\n",
        "while i<10000:\n",
        "  state=goal\n",
        "  while state==goal:\n",
        "    state = random.randint(0,8)\n",
        "\n",
        "  I = 1\n",
        "  i+=1\n",
        "\n",
        "  #policy_loss = []\n",
        "  #value_loss = []\n",
        "  if i%1000==0:\n",
        "    print(i)\n",
        "  while state!=goal:\n",
        "    x = state//3\n",
        "    y = state%3\n",
        "    state_tensor = torch.tensor((x,y), dtype=torch.float32)\n",
        "    action_probs = policy_net(state_tensor)\n",
        "    action_dist = torch.distributions.Categorical(action_probs) #categorical keeps track of action probs , log probs .\n",
        "    action_index  = action_dist.sample() #sample action\n",
        "    action = action_map[action_index.item()]\n",
        "    log_prob = action_dist.log_prob(action_index)#log probability\n",
        "    if random.random()>0.7:\n",
        "      action = random.choice(action_map)\n",
        "\n",
        "    newx,newy = grid.get_next_state((x,y),action)\n",
        "    #print(state,\" , \", action,\" , \",id)\n",
        "    reward = grid.get_reward((newx,newy))\n",
        "    #print(action_probs)\n",
        "\n",
        "    newstate = newx*3+newy\n",
        "\n",
        "    Vs = value_net(torch.tensor((x,y), dtype=torch.float32))\n",
        "    Vs_next = value_net(torch.tensor((newx,newy), dtype=torch.float32)).detach()\n",
        "    delta = reward + gamma*Vs_next - Vs\n",
        "\n",
        "    value_loss= (delta**(2)).sum()\n",
        "    policy_loss=(-log_prob*delta.detach()*I)\n",
        "\n",
        "    optimizer1.zero_grad()\n",
        "    policy_loss.backward()\n",
        "    optimizer1.step()\n",
        "\n",
        "    optimizer2.zero_grad()\n",
        "    value_loss.backward()\n",
        "    optimizer2.step()\n",
        "\n",
        "    I*=gamma\n",
        "\n",
        "    state = newstate\n",
        "\n",
        "    if state == goal:\n",
        "      break\n",
        "\n",
        "print(i)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "unidxmPQOHC6",
        "outputId": "dfe2335a-1daa-4db4-d432-71e36b8bfad4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1000\n",
            "2000\n",
            "3000\n",
            "4000\n",
            "5000\n",
            "6000\n",
            "7000\n",
            "8000\n",
            "9000\n",
            "10000\n",
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(i)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jEJ72PTtXAuz",
        "outputId": "f3f69d9d-e6d7-4c5b-8028-2976c7ed08e8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "10000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in range(9):\n",
        "  x = i//3\n",
        "  y = i%3\n",
        "  state_tensor = torch.tensor((x,y), dtype=torch.float32)\n",
        "  action_probs = policy_net(state_tensor)\n",
        "\n",
        "  print(i)\n",
        "  print('----------------------------------------------')\n",
        "  print(\"Best Action :\", action_map[np.argmax(action_probs.detach().numpy())])\n",
        "  print(action_probs)\n",
        "  #print(action_probs)\n",
        "  #print(action_dist.logits)\n",
        "  #print(action_dist.probs)\n",
        "  #print(action_index)\n",
        "\n",
        "  #print(action)\n",
        "  #print(log_prob)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "t3TzAgHtW0uf",
        "outputId": "e92f3ae3-64b6-484e-c648-68333e848da0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0\n",
            "----------------------------------------------\n",
            "Best Action : right\n",
            "tensor([0.1359, 0.0228, 0.0248, 0.8166], grad_fn=<SoftmaxBackward0>)\n",
            "1\n",
            "----------------------------------------------\n",
            "Best Action : right\n",
            "tensor([0.1436, 0.0045, 0.0048, 0.8471], grad_fn=<SoftmaxBackward0>)\n",
            "2\n",
            "----------------------------------------------\n",
            "Best Action : right\n",
            "tensor([0.2304, 0.0009, 0.0010, 0.7677], grad_fn=<SoftmaxBackward0>)\n",
            "3\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([0.5902, 0.0115, 0.0119, 0.3864], grad_fn=<SoftmaxBackward0>)\n",
            "4\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([0.7396, 0.0018, 0.0020, 0.2566], grad_fn=<SoftmaxBackward0>)\n",
            "5\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([8.3611e-01, 2.5129e-04, 2.8964e-04, 1.6335e-01],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "6\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([0.9411, 0.0015, 0.0015, 0.0559], grad_fn=<SoftmaxBackward0>)\n",
            "7\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([9.7247e-01, 1.7974e-04, 1.9634e-04, 2.7157e-02],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "8\n",
            "----------------------------------------------\n",
            "Best Action : up\n",
            "tensor([9.8671e-01, 2.2456e-05, 2.6727e-05, 1.3242e-02],\n",
            "       grad_fn=<SoftmaxBackward0>)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Replicate eligibility traces\n",
        "\n",
        "where zw stores value weight modification\n",
        "and z_theta stores policy weight modification"
      ],
      "metadata": {
        "id": "ElxGvio0Xs_t"
      }
    }
  ]
}